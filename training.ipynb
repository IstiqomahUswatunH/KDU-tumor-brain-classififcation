{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install -q deeplake torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRKyVeMCR_i2",
        "outputId": "a33452aa-017f-4084-e6ac-dff16dd23548"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.2/588.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.2/16.2 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/cu111/torch_stable.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsoX80DMUTlL",
        "outputId": "8966f0e6-8489-464a-852d-8af67b199dae"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/cu111/torch_stable.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiNU2_HhRvLC"
      },
      "source": [
        "# load data from deeplake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TBXax2N6RvLN"
      },
      "outputs": [],
      "source": [
        "import deeplake\n",
        "import os\n",
        "\n",
        "os.environ['ACTIVELOOP_TOKEN']='eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjE3NDU3NCwiZXhwIjoxNzA5MTk4NTU0fQ.eyJpZCI6Imhhc2FuYWgifQ.97shygl_oh8OkY4VC58ukoq_IhEDAi62IQSC4NZ9N63qtXpyH99EAacK1lhkFcQR2xaJOBmDafIUdgXc-l52Cw'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrg-77q5RvLU",
        "outputId": "eb22e0fc-f720-4ef5-e559-bf1e4e517980"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "-"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/hasanah/brain_tumor_train\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "|"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hub://hasanah/brain_tumor_train loaded successfully.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "-"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/hasanah/brain_tumor_val\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hub://hasanah/brain_tumor_val loaded successfully.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "-"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/hasanah/brain_tumor_test\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hub://hasanah/brain_tumor_test loaded successfully.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r \r\r\r"
          ]
        }
      ],
      "source": [
        "# load the dataset\n",
        "ds_train = deeplake.load('hub://hasanah/brain_tumor_train')\n",
        "ds_val = deeplake.load('hub://hasanah/brain_tumor_val')\n",
        "ds_test = deeplake.load('hub://hasanah/brain_tumor_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIw_jl5uRvLY",
        "outputId": "30317e9e-c979-4178-e26a-5714bf048f8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images in train dataset: 174\n",
            "Number of images in validation dataset: 51\n",
            "Number of images in validation dataset: 26\n"
          ]
        }
      ],
      "source": [
        "# Get the number of images in train and validation datasets\n",
        "num_train_images = len(ds_train)\n",
        "num_val_images = len(ds_val)\n",
        "num_test_images = len(ds_test)\n",
        "\n",
        "print(f\"Number of images in train dataset: {num_train_images}\")\n",
        "print(f\"Number of images in validation dataset: {num_val_images}\")\n",
        "print(f\"Number of images in validation dataset: {num_test_images}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "typkjBc1RvLZ"
      },
      "source": [
        "# preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "gfFWS_iBRvLa"
      },
      "outputs": [],
      "source": [
        "import deeplake\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os, time\n",
        "import torch\n",
        "from torchvision import transforms, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "PB_ApEI3RvLb"
      },
      "outputs": [],
      "source": [
        "tform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomRotation(20), # Image augmentation\n",
        "    transforms.Grayscale(num_output_channels=1),  #being 1 channel\n",
        "    transforms.ToTensor(), # Must convert to pytorch tensor for subsequent operations to run\n",
        "    transforms.Normalize([0.5], [0.5]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "LqNBhCRXRvLc"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "# Since torchvision transforms expect PIL images, we use the 'pil' decode_method for the 'images' tensor. This is much faster than running ToPILImage inside the transform\n",
        "train_loader = ds_train.pytorch(num_workers = 0, shuffle = True, transform = {'images': tform, 'labels': None}, batch_size = batch_size, decode_method = {'images': 'pil'})\n",
        "val_loader = ds_val.pytorch(num_workers = 0, transform = {'images': tform, 'labels': None}, batch_size = batch_size, decode_method = {'images': 'pil'})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYYJ8YRxSlkf",
        "outputId": "8212d51b-c9da-4757-ff8b-f93573058749"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGsU8pIQRvLe",
        "outputId": "50a435d8-93f1-4a78-bc28-5270d16fd139"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "from torchvision import models\n",
        "import torch\n",
        "\n",
        "# Use a pre-trained ResNet18\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Convert model to grayscale\n",
        "model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "# Update the fully connected layer based on the number of classes in the dataset\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, len(ds_train.labels.info.class_names))\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Specity the loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "2JGGMiAuRvLi"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, optimizer, data_loader, device):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Zero the performance stats for each epoch\n",
        "    running_loss = 0.0\n",
        "    start_time = time.time()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    for i, data in enumerate(data_loader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs = data['images']\n",
        "        labels = torch.squeeze(data['labels'])\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs.float())\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        accuracy = 100 * correct / total\n",
        "\n",
        "        # Print performance statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 0:    # print every 10 batches\n",
        "            batch_time = time.time()\n",
        "            speed = (i+1)/(batch_time-start_time)\n",
        "            print('[%5d] loss: %.3f, speed: %.2f, accuracy: %.2f %%' %\n",
        "                  (i, running_loss, speed, accuracy))\n",
        "\n",
        "            running_loss = 0.0\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "\n",
        "def validation_model(model, data_loader, device):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    start_time = time.time()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(data_loader):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs = data['images']\n",
        "            labels = torch.squeeze(data['labels'])\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs.float())\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        accuracy = 100 * correct / total\n",
        "\n",
        "        print('validation accuracy: %.1f %%' %(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ci5tyWMHRvLl",
        "outputId": "5bdc12ab-9865-4500-88d6-335343ae6de9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------ Training Epoch 1 ------------------\n",
            "[    0] loss: 0.591, speed: 0.42, accuracy: 71.88 %\n",
            "validation accuracy: 49.0 %\n",
            "------------------ Training Epoch 2 ------------------\n",
            "[    0] loss: 0.556, speed: 0.64, accuracy: 71.88 %\n",
            "validation accuracy: 68.6 %\n",
            "------------------ Training Epoch 3 ------------------\n",
            "[    0] loss: 0.529, speed: 0.37, accuracy: 71.88 %\n",
            "validation accuracy: 74.5 %\n",
            "------------------ Training Epoch 4 ------------------\n",
            "[    0] loss: 0.472, speed: 0.64, accuracy: 71.88 %\n",
            "validation accuracy: 76.5 %\n",
            "------------------ Training Epoch 5 ------------------\n",
            "[    0] loss: 0.531, speed: 0.79, accuracy: 78.12 %\n",
            "validation accuracy: 80.4 %\n",
            "------------------ Training Epoch 6 ------------------\n",
            "[    0] loss: 0.471, speed: 0.35, accuracy: 78.12 %\n",
            "validation accuracy: 76.5 %\n",
            "------------------ Training Epoch 7 ------------------\n",
            "[    0] loss: 0.368, speed: 0.21, accuracy: 93.75 %\n",
            "validation accuracy: 86.3 %\n",
            "------------------ Training Epoch 8 ------------------\n",
            "[    0] loss: 0.444, speed: 0.21, accuracy: 84.38 %\n",
            "validation accuracy: 84.3 %\n",
            "------------------ Training Epoch 9 ------------------\n",
            "[    0] loss: 0.411, speed: 0.17, accuracy: 87.50 %\n",
            "validation accuracy: 80.4 %\n",
            "------------------ Training Epoch 10 ------------------\n",
            "[    0] loss: 0.495, speed: 0.30, accuracy: 75.00 %\n",
            "validation accuracy: 84.3 %\n",
            "------------------ Training Epoch 11 ------------------\n",
            "[    0] loss: 0.383, speed: 0.21, accuracy: 78.12 %\n",
            "validation accuracy: 86.3 %\n",
            "------------------ Training Epoch 12 ------------------\n",
            "[    0] loss: 0.490, speed: 0.39, accuracy: 84.38 %\n",
            "validation accuracy: 88.2 %\n",
            "------------------ Training Epoch 13 ------------------\n",
            "[    0] loss: 0.369, speed: 0.31, accuracy: 84.38 %\n",
            "validation accuracy: 86.3 %\n",
            "------------------ Training Epoch 14 ------------------\n",
            "[    0] loss: 0.365, speed: 0.28, accuracy: 87.50 %\n",
            "validation accuracy: 84.3 %\n",
            "------------------ Training Epoch 15 ------------------\n",
            "[    0] loss: 0.414, speed: 0.30, accuracy: 84.38 %\n",
            "validation accuracy: 88.2 %\n",
            "------------------ Training Epoch 16 ------------------\n",
            "[    0] loss: 0.406, speed: 0.29, accuracy: 75.00 %\n",
            "validation accuracy: 84.3 %\n",
            "------------------ Training Epoch 17 ------------------\n",
            "[    0] loss: 0.411, speed: 0.38, accuracy: 81.25 %\n",
            "validation accuracy: 84.3 %\n",
            "------------------ Training Epoch 18 ------------------\n",
            "[    0] loss: 0.360, speed: 0.25, accuracy: 90.62 %\n",
            "validation accuracy: 88.2 %\n",
            "------------------ Training Epoch 19 ------------------\n",
            "[    0] loss: 0.360, speed: 0.30, accuracy: 90.62 %\n",
            "validation accuracy: 90.2 %\n",
            "------------------ Training Epoch 20 ------------------\n",
            "[    0] loss: 0.346, speed: 0.25, accuracy: 87.50 %\n",
            "validation accuracy: 84.3 %\n",
            "------------------ Training Epoch 21 ------------------\n",
            "[    0] loss: 0.282, speed: 0.23, accuracy: 90.62 %\n",
            "validation accuracy: 84.3 %\n",
            "------------------ Training Epoch 22 ------------------\n",
            "[    0] loss: 0.390, speed: 0.27, accuracy: 81.25 %\n",
            "validation accuracy: 84.3 %\n",
            "------------------ Training Epoch 23 ------------------\n",
            "[    0] loss: 0.315, speed: 0.17, accuracy: 93.75 %\n",
            "validation accuracy: 86.3 %\n",
            "------------------ Training Epoch 24 ------------------\n",
            "[    0] loss: 0.274, speed: 0.28, accuracy: 100.00 %\n",
            "validation accuracy: 84.3 %\n",
            "------------------ Training Epoch 25 ------------------\n",
            "[    0] loss: 0.365, speed: 0.28, accuracy: 90.62 %\n",
            "validation accuracy: 88.2 %\n",
            "------------------ Training Epoch 26 ------------------\n",
            "[    0] loss: 0.344, speed: 0.33, accuracy: 78.12 %\n",
            "validation accuracy: 90.2 %\n",
            "------------------ Training Epoch 27 ------------------\n",
            "[    0] loss: 0.289, speed: 0.46, accuracy: 96.88 %\n",
            "validation accuracy: 86.3 %\n",
            "------------------ Training Epoch 28 ------------------\n",
            "[    0] loss: 0.314, speed: 0.33, accuracy: 90.62 %\n",
            "validation accuracy: 86.3 %\n",
            "------------------ Training Epoch 29 ------------------\n",
            "[    0] loss: 0.307, speed: 0.46, accuracy: 90.62 %\n",
            "validation accuracy: 88.2 %\n",
            "------------------ Training Epoch 30 ------------------\n",
            "[    0] loss: 0.226, speed: 0.18, accuracy: 96.88 %\n",
            "validation accuracy: 88.2 %\n",
            "------------------ Training Epoch 31 ------------------\n",
            "[    0] loss: 0.223, speed: 0.22, accuracy: 96.88 %\n",
            "validation accuracy: 86.3 %\n",
            "------------------ Training Epoch 32 ------------------\n",
            "[    0] loss: 0.276, speed: 0.25, accuracy: 93.75 %\n",
            "validation accuracy: 90.2 %\n",
            "------------------ Training Epoch 33 ------------------\n",
            "[    0] loss: 0.270, speed: 0.30, accuracy: 93.75 %\n",
            "validation accuracy: 84.3 %\n",
            "------------------ Training Epoch 34 ------------------\n",
            "[    0] loss: 0.248, speed: 0.30, accuracy: 93.75 %\n",
            "validation accuracy: 86.3 %\n",
            "------------------ Training Epoch 35 ------------------\n",
            "[    0] loss: 0.353, speed: 0.28, accuracy: 81.25 %\n",
            "validation accuracy: 90.2 %\n",
            "------------------ Training Epoch 36 ------------------\n",
            "[    0] loss: 0.208, speed: 0.21, accuracy: 93.75 %\n",
            "validation accuracy: 84.3 %\n",
            "------------------ Training Epoch 37 ------------------\n",
            "[    0] loss: 0.268, speed: 0.17, accuracy: 87.50 %\n",
            "validation accuracy: 90.2 %\n",
            "------------------ Training Epoch 38 ------------------\n",
            "[    0] loss: 0.218, speed: 0.25, accuracy: 93.75 %\n",
            "validation accuracy: 86.3 %\n",
            "------------------ Training Epoch 39 ------------------\n",
            "[    0] loss: 0.197, speed: 0.39, accuracy: 100.00 %\n",
            "validation accuracy: 90.2 %\n",
            "------------------ Training Epoch 40 ------------------\n",
            "[    0] loss: 0.191, speed: 0.36, accuracy: 96.88 %\n",
            "validation accuracy: 88.2 %\n",
            "------------------ Training Epoch 41 ------------------\n",
            "[    0] loss: 0.168, speed: 0.56, accuracy: 96.88 %\n",
            "validation accuracy: 88.2 %\n",
            "------------------ Training Epoch 42 ------------------\n",
            "[    0] loss: 0.168, speed: 0.53, accuracy: 96.88 %\n",
            "validation accuracy: 86.3 %\n",
            "------------------ Training Epoch 43 ------------------\n",
            "[    0] loss: 0.199, speed: 0.48, accuracy: 100.00 %\n",
            "validation accuracy: 86.3 %\n",
            "------------------ Training Epoch 44 ------------------\n",
            "[    0] loss: 0.212, speed: 0.42, accuracy: 96.88 %\n",
            "validation accuracy: 92.2 %\n",
            "------------------ Training Epoch 45 ------------------\n",
            "[    0] loss: 0.160, speed: 0.24, accuracy: 96.88 %\n",
            "validation accuracy: 88.2 %\n",
            "------------------ Training Epoch 46 ------------------\n",
            "[    0] loss: 0.169, speed: 0.28, accuracy: 100.00 %\n",
            "validation accuracy: 86.3 %\n",
            "------------------ Training Epoch 47 ------------------\n",
            "[    0] loss: 0.189, speed: 0.27, accuracy: 96.88 %\n",
            "validation accuracy: 90.2 %\n",
            "------------------ Training Epoch 48 ------------------\n",
            "[    0] loss: 0.207, speed: 0.19, accuracy: 96.88 %\n",
            "validation accuracy: 86.3 %\n",
            "------------------ Training Epoch 49 ------------------\n",
            "[    0] loss: 0.230, speed: 0.19, accuracy: 93.75 %\n",
            "validation accuracy: 88.2 %\n",
            "------------------ Training Epoch 50 ------------------\n",
            "[    0] loss: 0.185, speed: 0.20, accuracy: 96.88 %\n",
            "validation accuracy: 84.3 %\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "    print(\"------------------ Training Epoch {} ------------------\".format(epoch+1))\n",
        "    train_one_epoch(model, optimizer, train_loader, device)\n",
        "\n",
        "    validation_model(model, val_loader, device)\n",
        "\n",
        "print('Finished Training')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}